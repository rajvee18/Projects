{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import RandomState\n",
    "\n",
    "# Generate data\n",
    "x = np.linspace(1, 20, 100)\n",
    "np.random.seed(42)\n",
    "y = []\n",
    "\n",
    "for i in x:\n",
    "    if i < 5:\n",
    "        y.append(0)\n",
    "    elif 5 < i < 7.5:\n",
    "        lab = np.random.randint(2)\n",
    "        y.append(lab)\n",
    "    elif 7.5 < i < 12.5:\n",
    "        y.append(1)\n",
    "    elif 12.5 < i < 15:\n",
    "        lab = np.random.randint(2)\n",
    "        y.append(lab)\n",
    "    else:\n",
    "        y.append(0)\n",
    "\n",
    "y = np.array(y)\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(x, y, c=\"blue\", s=2)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "# Gradient descent function\n",
    "def gradient_descent(x, y):\n",
    "    m_curr = b_curr = 0\n",
    "    iterations = 100000\n",
    "    n = len(x)\n",
    "    learning_rate = 0.0002\n",
    "\n",
    "    for i in range(iterations):\n",
    "        y_predicted = m_curr * x + b_curr\n",
    "        cost = (1/n) * sum([val**2 for val in (y - y_predicted)])\n",
    "        md = -(2/n) * sum(x * (y - y_predicted))\n",
    "        bd = -(2/n) * sum(y - y_predicted)\n",
    "        m_curr -= learning_rate * md\n",
    "        b_curr -= learning_rate * bd\n",
    "\n",
    "    print(f\"m {m_curr}, b {b_curr}, iteration {i}, cost {cost}\")\n",
    "    return m_curr, b_curr, cost\n",
    "\n",
    "m, b, cost = gradient_descent(x, y)\n",
    "\n",
    "# Plot the linear regression line\n",
    "plt.plot(x, m * x + b, color='green')\n",
    "\n",
    "# Calculate mean and variance\n",
    "mean = np.mean(x)\n",
    "std = np.std(x)\n",
    "var = std**2\n",
    "\n",
    "# Calculate Gaussian values\n",
    "y_gaussian = (np.exp((-1 * (m * x + b - mean)**2) / (2 * var))) / ((2 * np.pi * var)**0.5)\n",
    "plt.scatter(x, y_gaussian, color='red')\n",
    "\n",
    "# Initialize m and b for NLL\n",
    "m = 0\n",
    "b = 0\n",
    "\n",
    "# Negative log-likelihood function\n",
    "def gaussian_nll(b, m):\n",
    "    pred_y = (np.exp((-1 * (m * x + b - mean)**2) / (2 * var))) / ((2 * np.pi * var)**0.5)\n",
    "    loss = 0.5 * np.sum(np.log(2 * np.pi * var) + ((y - pred_y)**2) / var)\n",
    "    return loss\n",
    "\n",
    "# Gradient descent for NLL\n",
    "iterations = 5000\n",
    "learning_rate = 0.001\n",
    "\n",
    "for i in range(iterations):\n",
    "    y_pred = (np.exp((-1 * (m * x + b - mean)**2) / (2 * var))) / ((2 * np.pi * var)**0.5)\n",
    "    md = np.sum((y - y_pred) * (x * (m * x + b - mean)) / var)\n",
    "    bd = np.sum((y - y_pred) * ((m * x + b - mean)) / var)\n",
    "\n",
    "    b -= learning_rate * bd\n",
    "    m -= learning_rate * md\n",
    "    loss = gaussian_nll(b, m)\n",
    "\n",
    "    if i % 100 == 0:  # Print loss every 100 iterations\n",
    "        print(loss)\n",
    "\n",
    "print(f'm {m}, b {b}, loss {loss}')\n",
    "\n",
    "# Final plot\n",
    "plt.scatter(x, y_gaussian, color='red')\n",
    "plt.scatter(x, y, c=\"blue\", s=2)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
